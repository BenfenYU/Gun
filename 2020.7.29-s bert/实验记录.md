# 实验记录

## 数据分布

### 训练数据

- 客户的数据集从类别1到类别6数据量分别为：[687, 2001, 540, 1923, 4496, 1883]
- [sentense transformer](https://github.com/UKPLab/sentence-transformers)（孪生bert的paper）的训练集从类别1到类别3数据量为：[314090, 314315, 313664]

## 使用bert-base-multilingual-uncased-pytorch_model

### 下载

[transformers模型的官网](https://huggingface.co/models)，注意包括三个部分 -> 模型、词典和配置文件。



## 使用bert-base-chinese-pytorch_model

### 下载

同上

#### 使用softmax调优（30 epoch）：

- 训练中在验证集上的精度：

![image-20200730215635311](../../../../notes-of-a-postgraduate/images/image-20200730215635311.png)

- 训练中的训练集loss：

![image-20200730215730263](../../../../notes-of-a-postgraduate/images/image-20200730215730263.png)

- 模型存放的文件夹名称：

​				training_nli_.-pretrained_model-bert-base-chinese-2020-07-30_15-59-13

#### 使用Label smoothing + softmax（rate为0.1）：

- 训练中的训练集loss：

![image-20200730223401722](../../../../notes-of-a-postgraduate/images/image-20200730223401722.png)

- 训练中在验证集上的精度：

![image-20200730223450914](../../../../notes-of-a-postgraduate/images/image-20200730223450914.png)

- 模型存放的文件夹名称：

  training_nli_.-pretrained_model-bert-base-chinese-2020-07-30_22-16-03

## 使用bert-base-uncased

- 模型存放的文件夹名称：

  training_nli_.-pretrained_model-bert-base-uncased-2020-07-31_10-58-03
  
  
  
  训练中的训练集loss：
  
  ![image-20200802211025330](../../../../notes-of-a-postgraduate/images/image-20200802211025330.png)
  
- 训练中在验证集上的精度：

![image-20200802211040045](../../../../notes-of-a-postgraduate/images/image-20200802211040045.png)

